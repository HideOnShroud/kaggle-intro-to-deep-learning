# Intro to Deep Learning

This repository contains a collection of Jupyter Notebooks and exercises from the Kaggle course "Intro to Deep Learning". The course focuses on using TensorFlow and Keras to build and train neural networks for structured data.

![Certificate](/Guga%20Natroshvili%20-%20Intro%20to%20Deep%20Learning.png)  <!-- Ensure this path is correct -->

## Overview

Deep learning is a subset of machine learning that uses neural networks with many layers. This repository documents my learning journey through various deep learning techniques and applications, covering foundational concepts to more advanced topics.

## Table of Contents

1. [A Single Neuron](#a-single-neuron)
2. [Deep Neural Networks](#deep-neural-networks)
3. [Stochastic Gradient Descent](#stochastic-gradient-descent)
4. [Overfitting and Underfitting](#overfitting-and-underfitting)
5. [Dropout and Batch Normalization](#dropout-and-batch-normalization)
6. [Binary Classification](#binary-classification)

## A Single Neuron

**File**: `exercise-a-single-neuron.ipynb`

This notebook introduces the concept of a linear unit, the basic building block of neural networks. Topics covered include:
- Understanding linear units
- Implementing a single neuron using TensorFlow and Keras

## Deep Neural Networks

**File**: `exercise-deep-neural-networks.ipynb`

This notebook explores adding hidden layers to neural networks to uncover complex relationships in data. Key topics include:
- Building deep neural networks
- Using hidden layers to improve model performance

## Stochastic Gradient Descent

**File**: `exercise-stochastic-gradient-descent.ipynb`

Learn how to train neural networks using Keras and TensorFlow with stochastic gradient descent. This notebook covers:
- Basics of stochastic gradient descent
- Training your first neural network

## Overfitting and Underfitting

**File**: `exercise-overfitting-and-underfitting.ipynb`

This notebook focuses on improving model performance by addressing overfitting and underfitting. Topics include:
- Understanding overfitting and underfitting
- Techniques like early stopping and adding extra capacity

## Dropout and Batch Normalization

**File**: `exercise-dropout-and-batch-normalization.ipynb`

Explore how dropout and batch normalization layers can prevent overfitting and stabilize training. This notebook covers:
- Implementing dropout layers
- Using batch normalization for better training stability

## Binary Classification

**File**: `exercise-binary-classification.ipynb`

Apply deep learning techniques to the common task of binary classification. Key topics include:
- Building and training binary classification models
- Evaluating model performance on binary classification tasks

## Contributing

This repository is primarily for personal learning, but contributions and feedback are welcome. If you have suggestions or improvements, feel free to open an issue or create a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.
